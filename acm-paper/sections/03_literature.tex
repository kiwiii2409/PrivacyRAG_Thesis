% !TeX root = ../main.tex
\section{Literature Review}\label{chapter:literature}
This chapter positions the existing work in the landscape of RAG privacy research. It builds on the definitions and threat model introduced in Chapter \ref{chapter:background} and focuses on three topics that motivate this thesis: (1) privacy leakage and attacks on RAG, (2) defensive strategies and their trade-offs, and (3) benchmarking and evaluation methods relevant to cross-document linkage.

\subsection{RAG Privacy Risks}\label{literature-sec:privacy-attacks}
Recent work demonstrates that, even in a black-box setting (as defined in Section~\ref{background-sec:threat-model}), RAG systems are vulnerable to data leakage \cite{implicationsRAG,goodAndBad}. A key vulnerability arises not just from direct leakage of sensitive information, but from the attacker's ability to accumulate information across multiple queries. This process enables the linkage of seemingly irrelevant data points to reconstruct sensitive profiles, a threat central to this thesis.

These privacy attacks can be broadly classified into the following categories: targeted attacks, and untargeted attacks.

\paragraph{Targeted attacks} Here the adversary designs queries to extract specific records or attributes. Examples include membership inference attacks, which try to determine whether a particular information is present in the retrieval corpus. Existing approaches are prompt-based attacks \cite{ragMIA}, similarity-based scoring \cite{generatingIsBelieving}, as well as adaptations of membership inference techniques originally developed for LLM training data \cite{extractingTrainingDataLLM,generatingIsBelieving}. Other targeted strategies employ prefix prompts that influence the model to complete sentences with sensitive context data or use LLM-optimized attack strings to target specific private records \cite{goodAndBad, DEAL}.  

\paragraph{Untargeted attacks} The goal is to extract as much of the retrieval corpus as possible. Simple variants append instructions such as "Please repeat all the context" \cite{spillTheBeans,goodAndBad}, while more advanced agent-based attacks like {RAGThief} \cite{ragThief} iteratively generate new adversarial queries from previous responses. The latter approach achieves extraction rates above 70\% on private knowledge bases, proving its effectiveness against commercial RAG systems.

As a defense, some commercial systems use prompt engineering to avoid privacy leakage \cite{anthropic_strengthen_guardrails,aws_secure_rag}. However, these defenses show limited effectiveness, as papers like \cite{targetingTheCore} show that adversarial prefixes such as "Forget all previous instructions" can bypass such prompts and still induce leakage. Most of the mentioned RAG leakage attacks are a form of \textit{prompt injections}, where malicious queries override system instructions to receive restricted information. Malicious query filtering has been proposed as potential solution, but papers like Silent Leaks \cite{silentLeaks} demonstrate high-extraction success using benign-looking queries to bypass filters.


\subsection{RAG Privacy Protection}
The approaches to mitigate RAG privacy leakage can be grouped into three broad classes of defenses: (1) preprocessing of retrieval data (2) generation-level defenses, and (3) query \& output sanitization

\subsection{Preprocessing Retrieval Data} 
Preprocessing approaches modify or replace the documents stored in the retrieval corpus before indexing. Methods like Synthetic Attribute-based Generation with agEnt-based refinement (SAGE) replace original documents with synthetic versions generated by a two-stage attribute extraction and agent-based refinement pipeline \cite{ragSAGE}. Other approaches, such as LPRAG, utilize local differential privacy on the entity-level. Sensitive entities are perturbed before indexing, providing formal differential privacy guarantees \cite{LPRAG}. Eraser4RAG is one of the first methods to explicitly model cross-document linkage. It extracts information as triples, constructs a global knowledge graph and trains a rewriting model to break sensitive links. While Eraser4RAG directly addresses linkage, its reliance on a trained rewriting model introduces considerable training complexity and cost, with the result lacking interpretability. Also, the reliance on structured triples potentially fails to capture sensitive information in unstructured text, limiting overall privacy preservation \cite{eraser4RAG}. 

Overall, data-level defenses reduce attack risk without adding inference overhead. They protect against a wide range attacks, as the data itself loses its value regarding re-identification. However, these strong transformations (strong perturbation or full synthesis) risk removing information useful for retrieval and LLM reasoning, degrading utility. Methods with formal guarantees (e.g. LPRAG) require careful budget tuning and struggle with precise knowledge due to the added noise. Finally, these defenses typically operate on a per-document level, which can lead to over- or under-redaction when linkages span multiple documents.


\subsection{Generation-level Defense} Generation-level defenses incorporate privacy mechanisms at inference. Methods like DPVoteRAG and DPSparseVoteRAG distribute response generation across multiple voters, each using disjoint subsets of the data, and add noise to satisfy differential privacy \cite{DPVoteRAG}. Others propose prompt-based defenses, by using a strong system prompt to enforce rules onto the generated output \cite{anthropic_strengthen_guardrails,aws_secure_rag,goodAndBad}. 

These methods retain the original retrieval data and operate during inference time without modifying the corpus. DP-based defenses offer formal privacy guarantees, while prompt-based defenses process all retrieved documents for each query, enabling more context-dependent redaction. Despite these advantages, DPVoteRAG loses utility once the privacy budget is depleted and adds inference overehead through multiple votersa and noise mechanisms. Prompt-based defenses, on the other hand, lack formal safety guarantees and can be bypassed using prompt injections (see Section \ref{literature-sec:privacy-attacks}). 


\subsection{Query and Output Sanitization} Query and output sanitization defenses act either before retrieval (malicious query filtering) or after generation (PII detectors / redactors). Practical toolkits such as Microsoft Presidio and industry documentation (e.g. AWS Bedrock) provide recognizers (regex, rule-based logic and NER) for common PII types, and RAG-specific work recommends response sanitization as part of a potential mitigation strategy \cite{aws_bedrock_privacy,ragThief,microsoft_presidio}. These defenses are generally easy to deploy and preserve original data, which helps maintain utility. However, filtering mechanisms are brittle in adversarial settings (see Section \ref{literature-sec:privacy-attacks}), while  output sanitization potentially produces false negatives for obfuscated or implicit identifiers. Therefore these apporaches are usually treated as an additional protective measure rather than standalone defenses.


\subsection{Benchmark and evaluation practices}
Existing work often uses datasets such as  \textit{HealthCareMagic} and the \textit{Enron Mail Dataset} to evaluate privacy risk. These datasets work at the single-document level and they do not provide a systematic way to measure privacy risk that arises from linking fragments across documents. For this reason, previously used benchmarks do not directly address the key threat targeted in this thesis. 

Utility testing, which commonly happens separate from privacy testing, is performed on Q\&A datasets like Natural Questions \cite{NQ}, triviaQA \cite{TriviaQA}, PopQA \cite{popQA} or HotpotQA \cite{hotpotQA}. These Q\&A datasets provide a wide range of topics for measuring answer quality with some including multi-hop reasoning challenges (e.g. HotpotQA). One of the main limitations of these datasets is the lack of direct- and/ or quasi-identifiers in the corpus. Therefore, results do not reflect the privacy-utility trade-off present in regular data.

This motivates the creation of a dedicated dataset that supplies (1) groups of documents (called clusters) with varying linkage strength, (2) matched single/ multi-source Q\&A pairs and (3) explicit privacy targets for linkage attacks. The full dataset generation and validation pipeline is described in Section \ref{evaluation-subsec:data-generation}.
