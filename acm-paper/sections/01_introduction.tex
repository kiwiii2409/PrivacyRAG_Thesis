% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\section{Introduction}\label{sec:introduction}

Retrieval Augmented Generation (RAG) systems combine a large language model with an external retrieval corpus to ground answers in up-to-date or domain specific knowledge. It has become a practical architecture for many real-world applications, from enterprise question answering to clinical decision support~\cite{chatDoctor}. By including relevant, domain specific context at inference time, RAG systems experience reduced hallucinations~\cite{ragNoHallucination} and improvements in factuality compared to LLM-only approaches~\cite{ragOrigin}.

At  the same time, appending retrieved passages to the model prompt introduces a new privacy risk: Personally Identifiable Information (PII) or other sensitive fragments stored in the retrieval corpus may be disclosed through generated outputs. Recent studies demonstrated the effectiveness of a variety of membership inference and extraction attacks that recover identifiers and fragments~\cite{ragMIA, ragThief,generatingIsBelieving,goodAndBad,DEAL}. 

This thesis addresses a specific, under-explored facet of the risk: \textit{cross-document linkage}. Many current privacy benchmarks and de-identification strategies focus on single-document redaction without considering the entire corpus~\cite{LPRAG,DPVoteRAG,ragSAGE}. However, practical re-identification risk often arises when numerous small fragments or \textit{quasi-identifiers}, distributed across multiple documents are combined. Classic re-identification studies and modern RAG attacks both indicate that aggregation of such fragments is possible and significantly increases identifiability~\cite{ragThief,netflixDeAnon, simpleDemographic}.

Recent work such as Eraser4RAG begins to address multi-document leakage by training rewriting models that remove private triples across documents. While this approach shows promise, it relies on model retraining and introduces challenges in interpretability and auditing, making it less straightforward to deploy~\cite{eraser4RAG}.

We propose CLAM (Cross-document Linkage-Aware Masking), an interpretable, configurable preprocessing approach that reduces cross-document linkage while reducing impact on utility. The CLAM pipeline (1) extracts candidate entities, scores them, (2) constructs a weighted document graph modeling cross-document links, and (3) performs minimal targeted value masking or pseudonymization to reduce both per-document and chain risk below configurable thresholds. Our approach aims to mask the smallest necessary set of links needed to achieve policy targets, thereby preserving utility wherever possible.

The primary contributions of this work are:
\begin{itemize}
  \item a risk-aware CLAM pipeline that prioritizes minimal masking based on corpus-wide risk scores derived from document-graph analysis
  \item a synthetic benchmark for cross-document linkage evaluation, which generates short, linked-document clusters, per-cluster privacy targets and single- and multi-source Q\&A pairs allowing systematic privacy and utility testing
  \item an evaluation framework combining automated black-box attacks, an LLM-based judge, and complementary automatic metrics to characterize the privacy-utility trade-off across baselines.
\end{itemize}

The evaluation compares CLAM against representative baselines (verbatim retrieval, SAGE-style synthetic replacements)~\cite{ragSAGE}. We perform a series of targeted black-box attacks (membership inference attacks, targeted extraction prompts) to probe for privacy leakage and use the Q\&A set to test utility.

The research questions driving this work are:
\begin{enumerate}
  \item Can a risk-aware preprocessing pipeline accurately model cross-document linkage risk in an unstructured retrieval corpus?
  \item Does CLAM reduce privacy leakage at both the document and cross-document (chain) levels compared to established baselines?
  \item Does CLAM preserve higher downstream utility (answer quality) than alternative privacy defenses while achieving comparable reductions in linkage risk?
\end{enumerate}



